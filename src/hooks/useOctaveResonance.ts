import { useRef, useEffect, useCallback } from 'react';

export interface ResonanceSettings {
    trimStart: number;      // Seconds to cut from start (Attack removal)
    fadeInDuration: number; // Seconds for fade-in
    fadeInCurve: number;    // Exponential factor (1 = linear, 5 = very steep)
    delayTime: number;      // Seconds to delay playback (Latency)
    masterGain: number;     // Volume of the harmonic
}

// Global Cache for AudioBuffers (Persists across component unmounts)
const globalAudioBuffers = new Map<string, AudioBuffer>();

// â˜… Voice Management System Types
interface ActiveVoice {
    id: number;
    source: AudioBufferSourceNode;
    gainNode: GainNode;
    startTime: number;
    noteName: string;
}

// Global Voice Counter for unique IDs
let voiceIdCounter = 0;

interface UseOctaveResonanceProps {
    getAudioContext?: () => AudioContext | null;
    getMasterGain?: () => GainNode | null;
}

export const useOctaveResonance = ({ getAudioContext, getMasterGain }: UseOctaveResonanceProps = {}) => {
    // We no longer strictly need a local ref if we use the getter, 
    // but for fallback we might still want one? 
    // Actually, to fix the mobile issue, we should AVOID creating a local context if one is supplied.
    const localAudioContextRef = useRef<AudioContext | null>(null);

    // â˜… Voice Management State (Local to hook, but works per component instance)
    // Note: If multiple components use this hook, we might need a global voice manager.
    // For now, Digipan3D is usually single instance, so a Ref is okay.
    // But to be safe across re-renders, we use a Ref to hold the array.
    const activeVoices = useRef<ActiveVoice[]>([]);

    // Dynamic Polyphony Limit (Detected on mount)
    const MAX_POLYPHONY = useRef(32); // Default Safe Limit

    useEffect(() => {
        // Simple User Agent Check for Dynamic Scaling
        const isMobile = /Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent);
        MAX_POLYPHONY.current = isMobile ? 32 : 64;
        console.log(`[Polyphony Manager] Configured Limit: ${MAX_POLYPHONY.current} voices (${isMobile ? 'Mobile' : 'Desktop'})`);
    }, []);

    // Helper: Voice Stealing Algorithm (LRU - Least Recently Used / Oldest)
    const managePolyphony = useCallback((currentTime: number) => {
        const limit = MAX_POLYPHONY.current;
        const voices = activeVoices.current;

        // If we have room, do nothing
        if (voices.length < limit) return;

        // We need to free up slots. How many?
        // Usually 1 is enough, but looping is safer if we are way over.
        const voicesToSteal = voices.length - limit + 1; // Steal 1 to make room for new one

        for (let i = 0; i < voicesToSteal; i++) {
            // The array is pushed in order, so index 0 is always the oldest.
            const oldestVoice = voices[0];
            if (oldestVoice) {
                // Smooth Steal: Quick fade out (10ms) to avoid click
                try {
                    const fadeTime = 0.01;
                    oldestVoice.gainNode.gain.cancelScheduledValues(currentTime);
                    oldestVoice.gainNode.gain.setValueAtTime(oldestVoice.gainNode.gain.value, currentTime);
                    oldestVoice.gainNode.gain.linearRampToValueAtTime(0, currentTime + fadeTime);
                    oldestVoice.source.stop(currentTime + fadeTime);
                } catch (e) {
                    // Ignore errors (node might be already stopped)
                }
                // Remove immediately from list
                voices.shift();
            }
        }
    }, []);

    // Helper to get the active context (Shared or Local)
    const getContext = useCallback(() => {
        if (getAudioContext) {
            const ctx = getAudioContext();
            if (ctx) return ctx;
        }
        return localAudioContextRef.current;
    }, [getAudioContext]);

    // Initialize Local AudioContext ONLY if no shared getter is provided (Legacy fallback)
    useEffect(() => {
        if (!getAudioContext) {
            const AudioContextClass = window.AudioContext || (window as any).webkitAudioContext;
            if (AudioContextClass) {
                localAudioContextRef.current = new AudioContext();
            }
        }
        return () => {
            if (localAudioContextRef.current) {
                localAudioContextRef.current.close().catch(e => console.warn("Failed to close AC", e));
            }
        };
    }, [getAudioContext]);

    // Load Audio Buffer (Global Cache)
    const loadBuffer = useCallback(async (noteName: string) => {
        // Return existing from Global Cache if available
        if (globalAudioBuffers.has(noteName)) {
            return globalAudioBuffers.get(noteName)!;
        }

        const ctx = getContext();
        if (!ctx) return null;

        try {
            // Skip resonance for Snare notes (they are percussive/dry)
            if (noteName.includes('Snare')) return null;

            let fileName = noteName.replace('#', '%23'); // Simple manual encode for #

            const response = await fetch(`/sounds/${fileName}.mp3`);
            const arrayBuffer = await response.arrayBuffer();

            // decoding audio data requires a context
            const audioBuffer = await ctx.decodeAudioData(arrayBuffer);

            globalAudioBuffers.set(noteName, audioBuffer);
            return audioBuffer;
        } catch (error) {
            console.error(`Failed to load resonance audio: ${noteName}`, error);
            return null;
        }
    }, [getContext]);

    const playResonantNote = useCallback(async (noteName: string, settings: ResonanceSettings) => {
        // â˜… [ë””ë²„ê·¸] ëª¨ë°”ì¼ í™˜ê²½ íŠ¹í™” í•˜ëª¨ë‹‰ ì¬ìƒ ë””ë²„ê¹…
        const isMobileDevice = typeof navigator !== 'undefined' && /Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent);
        const playStart = performance.now();
        console.log(`[Debug-Resonance] ===== í•˜ëª¨ë‹‰ ì¬ìƒ ì‹œì‘: ${noteName} =====`);
        console.log(`[Debug-Resonance] í™˜ê²½: ${isMobileDevice ? 'ğŸ“± ëª¨ë°”ì¼' : 'ğŸ’» ë°ìŠ¤í¬í†±'}`);

        const ctx = getContext();
        if (!ctx) {
            console.warn(`[Debug-Resonance] âš ï¸ AudioContext ì—†ìŒ!`);
            return;
        }

        console.log(`[Debug-Resonance] AudioContext ìƒíƒœ: ${ctx.state}`);
        console.log(`[Debug-Resonance] AudioContext sampleRate: ${ctx.sampleRate}Hz`);
        console.log(`[Debug-Resonance] AudioContext currentTime: ${ctx.currentTime.toFixed(4)}s`);
        console.log(`[Debug-Resonance] AudioContext baseLatency: ${ctx.baseLatency ?? 'N/A'}s`);

        if (ctx.state === 'suspended') {
            console.log(`[Debug-Resonance] AudioContext resume ì‹œë„...`);
            const resumeStart = performance.now();
            await ctx.resume();
            console.log(`[Debug-Resonance] AudioContext resume ì™„ë£Œ: ${(performance.now() - resumeStart).toFixed(1)}ms`);
        }

        // ë²„í¼ ë¡œë”© ì‹œê°„ ì¸¡ì •
        const bufferStart = performance.now();
        const buffer = await loadBuffer(noteName);
        const bufferLoadTime = performance.now() - bufferStart;

        if (!buffer) {
            console.warn(`[Debug-Resonance] âš ï¸ ë²„í¼ ë¡œë“œ ì‹¤íŒ¨: ${noteName}`);
            return;
        }

        console.log(`[Debug-Resonance] ë²„í¼ ë¡œë“œ: ${bufferLoadTime.toFixed(1)}ms (ìºì‹œ ${bufferLoadTime < 1 ? 'HIT âœ…' : 'MISS âŒ'})`);
        console.log(`[Debug-Resonance] ë²„í¼ ê¸¸ì´: ${buffer.duration.toFixed(2)}s, ì±„ë„: ${buffer.numberOfChannels}`);

        const source = ctx.createBufferSource();
        source.buffer = buffer;

        const gainNode = ctx.createGain();
        source.connect(gainNode);

        const masterGain = getMasterGain ? getMasterGain() : null;
        console.log(`[Debug-Resonance] Master Gain ì—°ê²°: ${masterGain ? 'âœ… Limiter ê²½ìœ ' : 'âŒ destination ì§ì ‘'}`);

        if (masterGain) {
            gainNode.connect(masterGain);
        } else {
            gainNode.connect(ctx.destination);
        }

        const now = ctx.currentTime;

        // â˜… [ì†”ë£¨ì…˜ A] ë°©ì–´ì  ì„ í–‰ ìŠ¤ì¼€ì¤„ë§ (Defensive Pre-scheduling)
        // iOS Safarië¥¼ ìœ„í•œ ì•ˆì „ ë§ˆì§„: ìµœì†Œ 50ms ì´ìƒ í™•ë³´
        const SAFE_MARGIN = 0.05; // 50ms
        const effectiveDelay = Math.max(settings.delayTime, SAFE_MARGIN);
        const startTime = now + effectiveDelay;

        // [ì •ë°€ ì§„ë‹¨] ìŠ¤ì¼€ì¤„ë§ ì—¬ìœ  ì‹œê°„ ì¸¡ì •
        const scheduleMargin = startTime - now;

        console.log(`[Debug-Resonance] [ì†”ë£¨ì…˜ A: ë°©ì–´ì  ì„ í–‰ ìŠ¤ì¼€ì¤„ë§]`);
        console.log(`[Debug-Resonance]   ctx.currentTime (now): ${now.toFixed(4)}s`);
        console.log(`[Debug-Resonance]   ì›ë˜ delay: ${settings.delayTime}s, ì ìš©ëœ delay: ${effectiveDelay}s`);
        console.log(`[Debug-Resonance]   startTime: ${startTime.toFixed(4)}s`);
        console.log(`[Debug-Resonance]   ì—¬ìœ  ë§ˆì§„: ${(scheduleMargin * 1000).toFixed(2)}ms ${scheduleMargin < 0.05 ? 'âš ï¸ ìœ„í—˜' : 'âœ… ì•ˆì „(50ms+)'}`)
        console.log(`[Debug-Resonance]   trimStart: ${settings.trimStart}s`);
        console.log(`[Debug-Resonance]   fadeIn: ${settings.fadeInDuration}s (curve: ${settings.fadeInCurve})`);
        console.log(`[Debug-Resonance]   targetGain: ${settings.masterGain}`);

        // â˜… [í•µì‹¬] Gain Node ì´ì¤‘ ì•µì»¤ë§ (Double Anchoring)
        // WebKit ë³´ê°„ ë²„ê·¸ ë°©ì§€: nowì™€ startTime ì–‘ìª½ì— setValueAtTime(0) ì„¤ì •
        console.log(`[Debug-Resonance] â˜…â˜…â˜… ì´ì¤‘ ì•µì»¤ë§ + ì§€ìˆ˜ í˜ì´ë“œ ë³µì› â˜…â˜…â˜…`);

        // 1. ì¦‰ì‹œ 0ìœ¼ë¡œ ê³ ì • (í˜„ì¬ ì‹œì ) - í‹± ë°©ì§€ í•µì‹¬
        gainNode.gain.setValueAtTime(0, now);
        console.log(`[Debug-Resonance]   â‘  setValueAtTime(0, now=${now.toFixed(4)}) - ì¦‰ì‹œ ê³ ì •`);

        // 2. ì¬ìƒ ì‹œì‘ ì‹œì ì—ë„ 0ìœ¼ë¡œ ì•µì»¤ (WebKit ë³´ê°„ ë²„ê·¸ ë°©ì§€)
        gainNode.gain.setValueAtTime(0, startTime);
        console.log(`[Debug-Resonance]   â‘¡ setValueAtTime(0, startTime=${startTime.toFixed(4)}) - ì•µì»¤`);

        const fadeEndTime = startTime + settings.fadeInDuration;

        // 3. í˜ì´ë“œ ê³¡ì„  ë¶„ê¸°: ì§€ìˆ˜(ëˆŒë¦¼) vs ì„ í˜•
        if (settings.fadeInCurve > 1) {
            // â˜… [ë³µì›] ì§€ìˆ˜ ê³¡ì„  í˜ì´ë“œ (ëˆŒë¦° ëª¨ì–‘)
            // exponentialRampëŠ” 0ì—ì„œ ì‹œì‘ ë¶ˆê°€ â†’ 0.001ì—ì„œ ì‹œì‘
            const EPSILON = 0.001;
            const rampStartTime = startTime + 0.001; // 1ms ì˜¤í”„ì…‹ í›„ ë¯¸ì„¸ê°’ ì„¤ì •

            gainNode.gain.setValueAtTime(EPSILON, rampStartTime);
            gainNode.gain.exponentialRampToValueAtTime(settings.masterGain, fadeEndTime);

            console.log(`[Debug-Resonance]   â‘¢ ì§€ìˆ˜ í˜ì´ë“œ ë³µì› (curve: ${settings.fadeInCurve})`);
            console.log(`[Debug-Resonance]      setValueAtTime(${EPSILON}, ${rampStartTime.toFixed(4)})`);
            console.log(`[Debug-Resonance]      exponentialRamp(${settings.masterGain}, ${fadeEndTime.toFixed(4)})`);
        } else {
            // ì„ í˜• ë¨í”„
            gainNode.gain.linearRampToValueAtTime(settings.masterGain, fadeEndTime);
            console.log(`[Debug-Resonance]   â‘¢ ì„ í˜• í˜ì´ë“œ (curve: ${settings.fadeInCurve})`);
            console.log(`[Debug-Resonance]      linearRamp(${settings.masterGain}, ${fadeEndTime.toFixed(4)})`);
        }

        // 4. ì†ŒìŠ¤ ì¬ìƒ ì˜ˆì•½
        source.start(startTime, settings.trimStart);
        console.log(`[Debug-Resonance]   â‘£ source.start(${startTime.toFixed(4)}, ${settings.trimStart}) í˜¸ì¶œë¨`);

        const totalTime = performance.now() - playStart;
        console.log(`[Debug-Resonance] ì´ ì²˜ë¦¬ ì‹œê°„: ${totalTime.toFixed(1)}ms`);
        console.log(`[Debug-Resonance] ===== í•˜ëª¨ë‹‰ ì¬ìƒ ìŠ¤ì¼€ì¤„ ì™„ë£Œ =====`);

        // â˜… [Voice Management] Register Active Voice
        // 1. Enforce Limit First
        managePolyphony(now);

        // 2. Add new voice
        const voiceId = ++voiceIdCounter;
        const voice: ActiveVoice = {
            id: voiceId,
            source: source,
            gainNode: gainNode,
            startTime: startTime,
            noteName: noteName
        };
        activeVoices.current.push(voice);

        // 3. Self-cleanup on ended
        source.onended = () => {
            // Remove from active list
            activeVoices.current = activeVoices.current.filter(v => v.id !== voiceId);
            // Disconnect nodes to free memory
            // gainNode.disconnect(); // Optional, GC usually handles this but good practice
        };

    }, [getContext, loadBuffer, getMasterGain, managePolyphony]);

    // Smart Preloading Function
    const preloadNotes = useCallback(async (noteNames: string[]) => {
        // Initialize local if needed and no shared
        if (!getAudioContext && !localAudioContextRef.current) {
            const AudioContextClass = window.AudioContext || (window as any).webkitAudioContext;
            if (AudioContextClass) {
                localAudioContextRef.current = new AudioContext();
            }
        }

        // Parallel loading without blocking
        noteNames.forEach(note => {
            // Only load if not already in global cache
            if (!globalAudioBuffers.has(note)) {
                loadBuffer(note).catch(err => console.warn(`[Resonance] Preload failed for ${note}`, err));
            }
        });
    }, [loadBuffer, getAudioContext]);

    return {
        playResonantNote,
        preloadNotes
    };
};
